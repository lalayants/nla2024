{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yqFYdFCYTRW"
   },
   "source": [
    "# Problem Set 1 (140 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_abFGg2YM43"
   },
   "source": [
    "## Important information\n",
    "\n",
    "Please submit the single Jupyter Notebook file, where only Python and Markdown/LaTeX are used. Any hand-written solutions inserted by photos or in any other way are prohibitive and will not be graded. If you will have any questions about using Markdown, ask them!\n",
    "\n",
    "The works will be checked for plagiarism. The score will be divided by the number of similar works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gqKcERHlQVp"
   },
   "source": [
    "# Norms and numbers (40 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDZ8Z6CBlQVt"
   },
   "source": [
    "## Problem 1 (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot6YQhUNlQVu"
   },
   "source": [
    "### Problem 1.1 (1 pts)\n",
    "- (1 pts) For $1 \\leq p \\leq q \\leq \\infty$, show that there always exists a constant $C = C(n,p,q) > 0$ such that $\\|x\\|_p \\leq C\\|x\\|_q$ for every $x \\in \\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22GspIxRdOpk"
   },
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7W_tY3WlQVv"
   },
   "source": [
    "### Problem 1.2 (4 pts)\n",
    "\n",
    "For any norm $\\|\\cdot\\|$ in finite dimentional space $\\mathbb{C}^{N}$ we define dual norm $\\|x\\|^{'} = \\sup\\left\\{\\left|y^{\\star} x\\right|:\\left\\|y\\right\\|=1\\right\\}$, where ${\\cdot}^{\\star}$ denotes transpose complex-conjugate vector.\n",
    "+ (2 pts). Prove that $\\|\\cdot\\|^{'}$ is a norm.\n",
    "+ (2 pts). Find dual norm for $l_p$ norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_Gvf4D6dS8q"
   },
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8I-Jls4lQVv"
   },
   "source": [
    "### Problem 1.3 (5 pts)\n",
    "\n",
    "Suppose $f(x)$ is a differentiable function receiving a vector input $x = [x_i]_{i=1}^d$ and returning a scalar. Then object $\\nabla_{x} f(x) = [\\frac{d}{dx_i}f(x)]_{i=1}^d$ is called gradient.\n",
    "\n",
    "Find gradient $\\nabla_{x} f(x)$ for:\n",
    "- (1 pts) $f(x) = (a, x)$\n",
    "- (1 pts) $f(x) = \\|Ax - b\\|_2^2$\n",
    "- (1 pts) $f(x) = \\|x\\|_2^2$\n",
    "- (1 pts) $f(x) = \\|x\\|_2$\n",
    "- (1 pts) $f(x) = \\log_e(1 + \\exp((a, x)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nj7yGEtSdTxa"
   },
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ahu7brVklQVw"
   },
   "source": [
    "## Problem 2 (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIrY-_i-lQVw"
   },
   "source": [
    "For this section one should provide full solution processes. Simple numerical answers would not be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IIES2uxlQVx"
   },
   "source": [
    "### Problem 2.1 (1 pts)\n",
    "Convert the given fixed point number in form of a binary string to a decimal scalar value. The integer part is 10 bits and the fractial part is 5 bits.\n",
    "$$1101010100101111$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynPNlvaEdVKC"
   },
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvhjFQnjlQVx"
   },
   "source": [
    "### Problem 2.2 (1 pts)\n",
    "Convert the given floating point number in form of a binary string to a decimal scalar value. The exponent part is 5 bits and the mantissa part is 8 bits.\n",
    "$$10100101101011$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9wM2NjwdVg6"
   },
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKrzdt2IlQVy"
   },
   "source": [
    "### Problem 2.3 (1 pts)\n",
    "Find an angle between two vectors $x = [8, 4, 1, 2]$ and $y = [2, -2, 1, 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKsMQg1ydV1y"
   },
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWAzc1jvlQVy"
   },
   "source": [
    "### Problem 2.4 (1 pts)\n",
    "For the given vector-to-scalar function $f(x) = c_1(x, x) + c_2(a, x) + c_3$, where $c_1 = 4, c_2 = 3, c_3 = 8$ and $a = [4, 2, -3, 4]$, find the corresponding gradient $\\nabla_x f(x)$ in a specific point $x = [-1, 6, 2, 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfIDnCJ7dWBa"
   },
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTk5SLH2lQVy"
   },
   "source": [
    "## Problem 3 (26 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amqE3iM4lQVy"
   },
   "source": [
    "### K-Nearest Neighbours for binary classification (26 pts)\n",
    "\n",
    "In this task, you will implement a simple version of the K-Nearest Neighbours (KNN) classifier for binary classification. You will explore different distance metrics and their connection to vector norms and understand their impact on the performance of the KNN algorithm.\n",
    "\n",
    "Additionally, you will answer some theoretical questions related to vector norms and their properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIdim3hClQVy"
   },
   "source": [
    "### Problem 3.1 Dataset Generation (5 pts)\n",
    "- (3 pts) Generate a synthetic dataset using the `make_blobs` function from `sklearn.datasets`.\n",
    "- (2 pts) Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWEzD4UolQVz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXJoMAvwlQV0"
   },
   "outputs": [],
   "source": [
    "# Generate a dataset and split it into train and test sets\n",
    "N = 2000                # number of samples\n",
    "dim = 2                 # number of features\n",
    "centers = 2             # number of classes\n",
    "cluster_std = 2.0       # standard deviation of the clusters\n",
    "center_box = (-3, 3)    # range of the clusters\n",
    "seed = 0xC0FFEE         # random seed for reproducibility\n",
    "test_size = 0.25        # proportion of the dataset to include in the test split\n",
    "\n",
    "X, y = # YOUR CODE\n",
    "X_train, X_test, y_train, y_test = # YOUR CODE\n",
    "\n",
    "# Plot the training set\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], 'ro', label='Class 0')\n",
    "plt.plot(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], 'bo', label='Class 1')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNbINY3PlQV0"
   },
   "source": [
    "### Problem 3.2 Implement Distance Metrics (9 pts)\n",
    "\n",
    "KNN uses a chosen distance metric to find the nearest neighbours. We will use following metrics:\n",
    "\n",
    "- (3 pts) **Manhattan Distance**\n",
    "- (3 pts) **Euclidean Distance**\n",
    "- (3 pts) **Cosine Distance**\n",
    "\n",
    "Implement each one instead of the provided placeholders.\n",
    "\n",
    "You must use numpy vector and matrix operations - it's strictly forbidden to use any forms of python loops (for, while, etc.). Implementation of each metric must follow respective placeholder description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wcey-eLAlQV0"
   },
   "outputs": [],
   "source": [
    "def manhattan_distance(x1, x2):\n",
    "    '''\n",
    "    Find the Manhattan distance between two sets of points\n",
    "        x1 - array of shape (n_samples, n_features) OR single point of shape (n_features)\n",
    "        x2 - array of shape (n_samples, n_features) OR single point of shape (n_features)\n",
    "\n",
    "    Returns:\n",
    "        distance - array of shape (n_samples) OR single value\n",
    "    '''\n",
    "    raise NotImplemented()\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    '''\n",
    "    Find the Eucledian distance between two sets of points\n",
    "        x1 - array of shape (n_samples, n_features) OR single point of shape (n_features)\n",
    "        x2 - array of shape (n_samples, n_features) OR single point of shape (n_features)\n",
    "\n",
    "    Returns:\n",
    "        distance - array of shape (n_samples) OR single value\n",
    "    '''\n",
    "    raise NotImplemented()\n",
    "\n",
    "def cosine_distance(x1, x2):\n",
    "    '''\n",
    "    Find the Cosine distance between two sets of points\n",
    "        x1 - array of shape (n_samples, n_features) OR single point of shape (n_features)\n",
    "        x2 - array of shape (n_samples, n_features) OR single point of shape (n_features)\n",
    "\n",
    "    Returns:\n",
    "        distance - array of shape (n_samples) OR single value\n",
    "    '''\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqfuK-F1lQV0"
   },
   "source": [
    "### Problem 3.3 KNN Classifier (8 pts)\n",
    "\n",
    "Implement a KNN classifier that can use any of the above distance metrics using provided template. The classifier should have methods for fitting the model to the training data, predicting the class labels for the test data, and finding the K-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2kpbDQolQV0"
   },
   "outputs": [],
   "source": [
    "class NearestNeighborsFinder:\n",
    "    def __init__(self, n_neighbors=5, metric=\"euclidean\"):\n",
    "        self.n_neighbors = n_neighbors\n",
    "\n",
    "        # choose metric function\n",
    "        if metric == \"euclidean\":\n",
    "            self._metric_func = euclidean_distance\n",
    "        elif metric == \"manhattan\":\n",
    "            self._metric_func = manhattan_distance\n",
    "        elif metric == \"cosine\":\n",
    "            self._metric_func = cosine_distance\n",
    "        else:\n",
    "            raise ValueError(\"Metric is not supported\", metric)\n",
    "        self.metric = metric\n",
    "\n",
    "    def kneighbors(self, X, return_distance=False):\n",
    "        '''\n",
    "        Find the K-neighbors of a point.\n",
    "            X - array of shape (n_queries, n_features)\n",
    "            return_distance - if True, return distances to the neighbors\n",
    "\n",
    "        Returns:\n",
    "            indices - array of shape (n_queries, n_neighbors)\n",
    "            distances - array of shape (n_queries, n_neighbors)\n",
    "        '''\n",
    "\n",
    "        raise NotImplemented()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        '''\n",
    "        Fit the model using X as training data\n",
    "            X - array of shape (n_samples, n_features)\n",
    "            y - array of shape (n_samples)\n",
    "\n",
    "        Returns:\n",
    "            self - fitted estimator\n",
    "        '''\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        neigbours_idx = self.kneighbors(X)\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qrtwRyDlQV1"
   },
   "source": [
    "### Problem 3.4 Evaluation\n",
    "\n",
    "- (2 pts) Test your estimator. Your implementation should yeild F1-score at least 0.8 for each metric.\n",
    "- (2 pts) Compare different decision boundaries. What is the difference between them? In which scenario we should use each metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7WhdgcRlQV1"
   },
   "outputs": [],
   "source": [
    "for metric in [\"euclidean\", \"manhattan\", \"cosine\"]:\n",
    "\n",
    "    # Test your estimator\n",
    "    estimator = NearestNeighborsFinder(n_neighbors=5, metric=metric)\n",
    "    score = f1_score(y_test, estimator.fit(X_train, y_train).predict(X_test))\n",
    "    print(f'F1-score For {metric} metric:', score)\n",
    "\n",
    "    assert score > 0.80, f\"Score for {metric} metric is too low. Check your implementation.\"\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    h = 0.25  # step size in the mesh\n",
    "    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.title(f'Decision boundary for {metric} metric')\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.plot(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], 'ro', label='Class 0')\n",
    "    plt.plot(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], 'bo', label='Class 1')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfZJI0WqQnkA"
   },
   "source": [
    "# Matrix norms (35 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgKPQBdhTC53"
   },
   "source": [
    "### Problem 1. Frobenius Norm computation (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let $U \\in \\mathbb{R}^{n \\times r}$ be an $n \\times r$ matrix with orthonormal columns $U^{\\top} U = I_r$. Let $V$ be an $m \\times r$ matrix.\n",
    "\n",
    "+ (3 pts). Propose an algorithm for the computation of $\\Vert A \\Vert^2_F$, where $A = U V^{\\top}$, and estimate its complexity with respect to $n, m, r$. How orthogonalization can be used?\n",
    "\n",
    "+ (3 pts). Consider the matrix $B = A \\circ A$, where $\\circ$ is the elementwise product of matrices (i.e., the elements of the matrix $B$ are squares of the elements of the matrix $A$). What is the maximal possible rank of the matrix $B$?\n",
    "\n",
    "+ (5 pts). Propose an algorithm for the computation of $\\Vert B \\Vert^2_F$ and estimate its complexity with respect to $n, m, r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agzvYb0DdYxh"
   },
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2. Unitary and orthogonal matrices (15 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icsTNMw_UEyn"
   },
   "source": [
    "+ (3 pts). Let $O\\in\\mathbb{R}^{n\\times n}$ be orthogonal matrix. Characterise explicitly all orthogonal matrices that are positive definite. You may start with $n=2$.\n",
    "\n",
    "+ (3 pts). Show that any unitary matrix from $\\mathbb{C}^{n\\times n}$ can be represented as a product of at most $n$ Householder reflectors.\n",
    "\n",
    "+ (5 pts). Give an example of $M\\in\\mathbb{R}^{n\\times n}$ matrix that preserves $l_\\infty$ norm but does not preserve $l_2$ norm; or prove that it is not possible. *Hint: you can try working with basis vectors*\n",
    "\n",
    "+ (6 pts). Do the same for $l_1$ norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Og2vq_u1dZLB"
   },
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-Nnup3iKsfm"
   },
   "source": [
    "### Problem 3. Orthogonal matrices (7 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A32Qce7mLamL"
   },
   "source": [
    "A square $n \\times n$ matrix $A$ is said to be in **upper Hessenberg form** if $a_{i,j}=0$ for all $i,j$ with $i > j+1$. Reducing matrix to this form will be useful for optimization of QR algorithm that can be used to get eigenvalues of the matrix.\n",
    "\n",
    "Now, we can use orthogonal matrices to reduce matrices to upper Hessenberg form. Implement your function that could do that for any $M\\in\\mathbb{R}^{n\\times n}$ using either Householder reflections or Givens rotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAh02ItsNGy_"
   },
   "source": [
    "+ (4 pts) Implement function Hessenberg_Transform which takes a real square matrix and returns transformed matrix in Hessenberg form\n",
    "+ (3 pts) Varing parameter $n$ generate random matrix of size $n \\times n$ and measure the computation time of Hessenberg_Transform function. Plot observed data: computation time versus $n$. Explain obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TOClZSfKcMJ"
   },
   "outputs": [],
   "source": [
    "def Hessenberg_Transform(A):\n",
    "  H = # your solution\n",
    "  return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBFN5rLGdbcA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19uYYBL2iWcY"
   },
   "source": [
    "# Random walk for matrix inverse (65 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZcW2QPWihAk"
   },
   "source": [
    "Our final goal will be to construct a random walk model that can be used to estimate individual elements of matrix inverse. We will need some technical details before we can proceed with the model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESIONm0Iig7G"
   },
   "source": [
    "## Problem 1. Spectral radius (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RGgXe4TigzA"
   },
   "source": [
    "Spectral radius $\\rho(A)$ of matrix $A$ is its maximal absolute eigenvalue:\n",
    "$\\rho(A) = \\max_i \\left|\\lambda_i(A)\\right|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DVNGn92m20e"
   },
   "source": [
    "**a.** (2 pts) Show that $\\rho(A)\\leq \\left(\\left\\|A^{k}\\right\\|\\right)^{\\frac{1}{k}}$ holds for arbitrary induced (aka subordinate or operator) norm $\\left\\|\\cdot\\right\\|$.\n",
    "\n",
    "**Hint:** use definition of eigenvalue and $\\left\\|A x\\right\\| \\leq \\left\\|A\\right\\| \\left\\|x\\right\\|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVkK3gW2kAD-"
   },
   "source": [
    "**Solution:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4IFqxAAm0Gk"
   },
   "source": [
    "**b.** (3 pts) Show that if $\\rho(A)<1$ matrix $I-A$ is invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7yxpoLLm9-N"
   },
   "source": [
    "**Solution:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieFM1uYFi7QV"
   },
   "source": [
    "## Problem 2. Neumann series (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRnco4rJlMzx"
   },
   "source": [
    "Neuman series of matrix $A$ is $\\sum_{k=0}^{\\infty} A^{k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ccZk7Wbn6xR"
   },
   "source": [
    "**a.** (3 pts) Suppose Neumann series for matrix $A$ converges. Demonstrate that $\\sum_{k=0}^{\\infty} A^{k} = (I - A)^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBBQazuboOcc"
   },
   "source": [
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAzkUutwpabu"
   },
   "source": [
    "**b.** (3 pts) Show that if $\\rho(A) < 1$ Neumann series converges absolutely, i.e., $\\sum_{k=0}^{\\infty} \\left\\|A^{k}\\right\\| < \\infty$ for any operator norm $\\left\\|\\cdot\\right\\|$.\n",
    "\n",
    "**Hint:** Use [Gelfand's formula](https://en.wikipedia.org/wiki/Spectral_radius#Gelfand's_formula) to bound a tail of the series above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaAGWDukppTP"
   },
   "source": [
    "**Solution:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2f0SQKQs1BU"
   },
   "source": [
    "**c.** (4 pts) Show that if Neumann series converges $\\rho(A) < 1$.\n",
    "\n",
    "**Hint:** What if $\\rho(A)\\geq1$? Can you reduce the problem to the analysis of an ordinaty series (not a matrix one)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxTRt3RLtnaV"
   },
   "source": [
    "**Solution:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9FQ-1E1cnTJ"
   },
   "source": [
    "## Problem 3. Random walk (7 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAdYI9-2y2Fs"
   },
   "source": [
    "Now we are ready to formulate matrix inversion with random walk. We will need several abstract definitions that will be later related to the inverse of a matrix.\n",
    "\n",
    "We assume that there are $N+1$ states which we denote $\\left\\{1, \\dots, N\\right\\} + \\left\\{T\\right\\}$ where $T$ is a **t**erminal state. The path (or walk) is a sequence of states that always ends at the terminal state. Two examples of paths are:\n",
    "\n",
    "1. Path that starts at state $1$ and reaches state $100$ in four steps before termination: $1\\rightarrow 5\\rightarrow 7\\rightarrow 10\\rightarrow 100\\rightarrow T$\n",
    "2. Path that starts at state $7$ and reaches state $7$ in two steps before termination: $7\\rightarrow 7\\rightarrow 7 \\rightarrow T$.\n",
    "\n",
    "To generate paths we suppose that we are equipped with matrix $p_{ij}$ that describes probabilities of transferring from state $i$ to state $j$ and vector $\\pi_i$ with probabilities of reaching terminal state $T$ from state $i$. Probabilities are positive and sum to one $\\pi_i + \\sum_{j}p_{ij} = 1$ for each state $i$. We also assume that the model is Markov, that is $p(i\\rightarrow j\\rightarrow k) = p(i\\rightarrow j) p(j\\rightarrow k)$.\n",
    "\n",
    "For example, consider $4$ states and\n",
    "$$\n",
    " p =\n",
    " \\begin{pmatrix}\n",
    " 0.2 & 0 & 0.4 & 0\\\\\n",
    " 0 & 0 & 0.4 & 0.5\\\\\n",
    " 0.1 & 0.1 & 0.1 & 0.1\\\\\n",
    " 0 & 0.3 & 0.2 & 0.2\\\\\n",
    " \\end{pmatrix},\n",
    " \\pi =\n",
    " \\begin{pmatrix}\n",
    " 0.4 \\\\\n",
    " 0.1 \\\\\n",
    " 0.6 \\\\\n",
    " 0.3\n",
    " \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "**a.** (1 pts) Find probability of path $1\\rightarrow 1\\rightarrow 1\\rightarrow 1 \\rightarrow T$\n",
    "\n",
    "**b.** (1 pts) Find probability of path $2 \\rightarrow 3 \\rightarrow 1 \\rightarrow 2 \\rightarrow 3 \\rightarrow T$\n",
    "\n",
    "**c.** (2 pts) Find probability that walk start at state $1$, performs three step and terminates at state $1$.\n",
    "\n",
    "**Hint:** Consider all paths of the form $1\\rightarrow \\star \\rightarrow \\star \\rightarrow 1 \\rightarrow T$.\n",
    "\n",
    "**d.** (3 pts) Consider an extended matrix\n",
    "\n",
    "$$\n",
    " P =\n",
    " \\begin{pmatrix}\n",
    " 0.2 & 0 & 0.4 & 0 & 0.4\\\\\n",
    " 0 & 0 & 0.4 & 0.5 & 0.1\\\\\n",
    " 0.1 & 0.1 & 0.1 & 0.1 & 0.6\\\\\n",
    " 0 & 0.3 & 0.2 & 0.2 & 0.3 \\\\\n",
    " 0 & 0 & 0 & 0 & 1\n",
    " \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This is a [stochastic matrix](https://en.wikipedia.org/wiki/Stochastic_matrix) that describes transitions $i\\rightarrow j$ and $i\\rightarrow T$ simultaneously with $T$ identified as state $5$.\n",
    "\n",
    "Show that for arbitrary stochastic matrix $P$, $\\left(P^{n}\\right)_{ij}$ contain probabilities of walks that start at state $i$ and end at state $j$ after $n$ steps.\n",
    "\n",
    "**Comment 1:** Matrix $P$ is a stochastic matrix if $P_{ij}\\geq 0$ and $\\sum_{j}P_{ij} = 1$.\n",
    "\n",
    "**Comment 2:** State $5$ is absorbing, so it is fine to consider walks of the form $1 \\rightarrow 2 \\rightarrow 5 \\rightarrow 5 \\rightarrow 5 \\rightarrow \\dots \\rightarrow 5$.\n",
    "\n",
    "**Comment 3:** You may find it helpful to first consider examples of walks with a small number of steps and matrix $P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oma2dACM2TIL"
   },
   "source": [
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhhAdK3Dcpr0"
   },
   "source": [
    "## Problem 4. Weighted random walk (5 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zRpxSSM5Cs-"
   },
   "source": [
    "In the previous problem we considered ordinary random walk. To perform useful computations we will add weights $v_{ij}$ that corresponds to transition from state $i$ to state $j$. The weight of walk $w = s_0 \\rightarrow s_1 \\rightarrow s_2 \\rightarrow \\dots \\rightarrow s_N \\rightarrow T$ is $V(w) = \\prod_{i=1}^{N-1} v_{s_i s_{i+1}}$.\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "  p =\n",
    "  \\begin{pmatrix}\n",
    "  0.2 & 0 & 0.4 & 0\\\\\n",
    "  0 & 0 & 0.4 & 0.5\\\\\n",
    "  0.1 & 0.1 & 0.1 & 0.1\\\\\n",
    "  0 & 0.3 & 0.2 & 0.2\\\\\n",
    "  \\end{pmatrix},\n",
    "  \\pi =\n",
    "  \\begin{pmatrix}\n",
    "  0.4 \\\\\n",
    "  0.1 \\\\\n",
    "  0.6 \\\\\n",
    "  0.3\n",
    "  \\end{pmatrix},\n",
    "  v = \\begin{pmatrix}\n",
    "  -1 & 0 & 2 & 0\\\\\n",
    "  0 & 0 & 3 & -7\\\\\n",
    "  0 & 1 & 1 & 1\\\\\n",
    "  0 & 3 & 2 & 2\\\\\n",
    "  \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "**a.** (1 pts) Find weight of walk $1\\rightarrow 1\\rightarrow 1\\rightarrow 1 \\rightarrow T$\n",
    "\n",
    "**b.** (1 pts) Find weight of walk $2 \\rightarrow 3 \\rightarrow 1 \\rightarrow 2 \\rightarrow 3 \\rightarrow T$\n",
    "\n",
    "**c.** (3 pts) Find mean value of random variable $V(w)$ given that walks $w$ start at state $1$, performs three step and terminates at state $1$, i.e., compute $\\sum_{w}V(w)p(w|s_0=1,s_{3}=1,s_{4}=T)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qI2Yn-MKFyVC"
   },
   "source": [
    "**Solution:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_6ZCs4xdMLl"
   },
   "source": [
    "## Problem 5. Random walk, Neumann series, and matrix inverse (12 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yezLMpMHtl6"
   },
   "source": [
    "\n",
    "Let $B = I - A \\in \\mathbb{R}^{N\\times N}$, $\\rho(A)<1$. Consider state space with $N$ ordinary states and one terminal state $T$. Let $p_{ij}$ and $v_{ij}$ are such that $A_{ij} = p_{ij} v_{ij}$, $\\pi_i = 1 - \\sum_{j} p_{ij} > 0$.\n",
    "\n",
    "Define random variable\n",
    "$$\n",
    "G_{ij}(i\\rightarrow \\star \\rightarrow \\dots \\rightarrow \\star \\rightarrow k \\rightarrow T) =\n",
    "\\begin{cases}\n",
    "&0,\\text{ if } k\\neq j;\\\\\n",
    "&\\frac{1}{\\pi_{j}}V(i\\rightarrow \\star \\rightarrow \\dots \\rightarrow \\star \\rightarrow k \\rightarrow T),\\text{ if } k=j.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**a.** (4 pts) Show that $\\mathbb{E}[G_{ij}] = \\left(B^{-1}\\right)_{ij}$. In other words, the average weight of all walks starting from state $i$ and reaching state $j$ right before termination equals $(B^{-1})_{ij} \\pi_{j}$. We know that the Neumann series converges absolutely when $\\rho(A)<1$. Is this property important for your proof?\n",
    "\n",
    "**Hint:** Use relations between: (i) $B$ and Neumann series of $A$; (ii) $A_{ij}$ and $v_{ij}$, $p_{ij}$; (iii) $A^{k}$ and random walks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6nz_eGhKIRE"
   },
   "source": [
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0F7AW6criaf"
   },
   "source": [
    "**b.** (4 pts) Let $C_{ij} = v_{ij} A_{ij} = p_{ij}v_{ij}^2$ and $D = (I - C)^{-1}$. Show that $\\sigma(G_{ij})^2 = \\mathbb{E}\\left[G_{ij}^2\\right] - \\left(\\mathbb{E}\\left[G_{ij}\\right]\\right)^2 = \\left(D^{-1}\\right)_{ij} \\big/ \\pi_{j} - \\left(\\left(B^{-1}\\right)_{ij}\\right)^2$ if $\\rho(C) < 1$ and $\\sigma(C_{ij})^2 = \\infty$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7ispIxpsmd4"
   },
   "source": [
    "**Solution:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MApdP5Wiy53t"
   },
   "source": [
    "**c.** (4 pts) Explain how $G_{ij}$ should be modified if one want to compute component of solution $\\left(B^{-1}v\\right)_{i}$ in place of an element of inverse matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HinhELMUzJjF"
   },
   "source": [
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clIoOQjIbplH"
   },
   "source": [
    "**Discussion.**\n",
    "\n",
    "Let's briefly discuss why the Monte Carlo method is interesting. There are a few reasons for that:\n",
    "\n",
    "1. It can be used to compute a single element of the inverse or a selected group of elements (hard to do with other methods!). For example, for certain applications in physics and chemistry, one needs to know the diagonal of the inverse matrix (see [A probing method for computing the diagonal of a matrix inverse](https://onlinelibrary.wiley.com/doi/abs/10.1002/nla.779)).\n",
    "2. By usual argument if variance is finite Monte Carlo converges as $\\frac{1}{\\sqrt{N}}$ where $N$ is a number of samples (see, for example, [an article on Monte Carlo integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration)).\n",
    "3. The method is massively parallel: weights of individual walks for any $ij$ can be computed independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0cuuzdVvbPz"
   },
   "source": [
    "## Problem 6. Implementation of Monte Carlo matrix inversion for dense matrix (26 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 5 shows us that it is possible to approximate the inverse of certain matrices by simulating random walks and computing empirical averages. In this problem, we are going to do just that for small dense linear problems without worrying too much about memory footprint and data structures.\n",
    "\n",
    "To proceed we need to choose $p_{ij}$ and $v_{ij}$. Assuming that $A\\in\\mathbb{R}^{m\\times m}$ is dense we will take $p_{ij} = \\frac{(1 - \\epsilon)}{m}$, $\\pi_i = \\epsilon$, $v_{ij} = A_{ij} \\frac{m}{1 - \\epsilon}$ for some $\\epsilon\\in(0, 1)$. This is not the best choice, but it is easier to implement. We also need some reasonable test cases of matrices $B$ suitable for the basic Monte Carlo we consider in this problem. We will use the following classes:\n",
    "\n",
    "1. **scaled unitary** $A = \\alpha U$ where $U = U^\\top$ and $\\alpha \\in (-1, 1)$. The random unitary matrix can be conveniently obtained from $G = QR$ where $G_{ij}$ are drawn independently from standard normal distribution.\n",
    "\n",
    "2. **scaled GOE** $A = \\frac{\\beta}{\\sqrt{2m}} \\left(G + G^\\top\\right)$ where $G\\in\\mathbb{R}^{m\\times m}$, $G_{ij}$ are drawn independently from standard normal distribution, $\\beta \\in \\left(-\\frac{1}{2}, \\frac{1}{2}\\right)$. For [Gaussian Orthogonal Ensemble](https://en.wikipedia.org/wiki/Random_matrix#Gaussian_ensembles) spectrum converges to [Wigner distribution](https://en.wikipedia.org/wiki/Wigner_semicircle_distribution) on $[-2, 2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PC3pw5_aXrqt"
   },
   "source": [
    "**a.** (7 pts) Implement Monte Carlo matrix inversion. Your code should be suitable:\n",
    "1. For matrices with different $m$.\n",
    "2. For different choices of $\\epsilon$.\n",
    "3. For arbitrary indices $ij$ of $\\left(B^{-1}\\right)_{ij}$.\n",
    "\n",
    "Provide code that demonstrate you implementation is working. For that you need to select some matrix for tests and compare exact $\\left(B^{-1}\\right)_{ij}$ with the one estimated with your implementation of Monte Carlo estimate for several choices of $i$, $j$, $m$, $\\epsilon$. Code for tests is available below.\n",
    "\n",
    "**Note that if you code is not running (for any reason), you get zero for this problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJO544Aqd1hM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_fTgIbtdidV"
   },
   "outputs": [],
   "source": [
    "def MC_solver(A, i, j, N_walks, eps):\n",
    "    m = A.shape[0]\n",
    "    inv_ij = 0\n",
    "\n",
    "    # YOUR CODE\n",
    "    # # # # # #\n",
    "\n",
    "    return inv_ij\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6QqXMDfgZXr9",
    "outputId": "247c2d3d-89c4-4a14-de10-5402abb61d40"
   },
   "outputs": [],
   "source": [
    "# test case 1\n",
    "\n",
    "np.random.seed(41)\n",
    "\n",
    "m = 5\n",
    "beta = 0.3\n",
    "\n",
    "G = np.random.randn(m, m)\n",
    "A = (G + G.T)*beta/np.sqrt(2*m)\n",
    "B = np.linalg.inv(np.eye(m) - A)\n",
    "\n",
    "eps = 0.1\n",
    "i = 0\n",
    "j = 4\n",
    "N_walks = 50000\n",
    "\n",
    "print(\"Test 1.1\", B[i, j], MC_solver(A, i, j, N_walks, eps))\n",
    "\n",
    "eps = 0.4\n",
    "i = 0\n",
    "j = 4\n",
    "N_walks = 50000\n",
    "\n",
    "print(\"Test 1.2\", B[i, j], MC_solver(A, i, j, N_walks, eps))\n",
    "\n",
    "eps = 0.4\n",
    "i = 1\n",
    "j = 1\n",
    "N_walks = 50000\n",
    "\n",
    "print(\"Test 1.3\", B[i, j], MC_solver(A, i, j, N_walks, eps))\n",
    "\n",
    "\n",
    "# test case 2\n",
    "\n",
    "np.random.seed(45)\n",
    "\n",
    "m = 7\n",
    "alpha = 0.25\n",
    "\n",
    "G = np.random.randn(m, m)\n",
    "Q = np.linalg.qr(G)[0]\n",
    "A = alpha * Q\n",
    "B = np.linalg.inv(np.eye(m) - A)\n",
    "\n",
    "eps = 0.1\n",
    "i = 5\n",
    "j = 4\n",
    "N_walks = 70000\n",
    "\n",
    "print(\"Test 2.1\", B[i, j], MC_solver(A, i, j, N_walks, eps))\n",
    "\n",
    "eps = 0.9\n",
    "i = 5\n",
    "j = 4\n",
    "N_walks = 70000\n",
    "\n",
    "print(\"Test 2.2\", B[i, j], MC_solver(A, i, j, N_walks, eps))\n",
    "\n",
    "eps = 0.2\n",
    "i = 5\n",
    "j = 5\n",
    "N_walks = 70000\n",
    "\n",
    "print(\"Test 2.3\", B[i, j], MC_solver(A, i, j, N_walks, eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klAuT6dtfqEG"
   },
   "source": [
    "**b.** (6 + 6 pts) Explain theoretically, why (and when) one would expect error to drop as $1 \\big / \\sqrt{N}$ where $N$ is a number of samples. Confirm this for small matrices experimentally. A good way to show this explicitly is to produce a plot alike one [available in wikipedia](https://en.wikipedia.org/wiki/Monte_Carlo_integration#Example) for Monte Carlo integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDTAs2zogHE_"
   },
   "source": [
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "uDdifBTggNCM",
    "outputId": "1f543537-ff30-4f3a-d23b-8f92966bd5c0"
   },
   "outputs": [],
   "source": [
    "np.random.seed(45)\n",
    "\n",
    "m = 7\n",
    "alpha = 0.1\n",
    "\n",
    "G = np.random.randn(m, m)\n",
    "Q = np.linalg.qr(G)[0]\n",
    "A = alpha * Q\n",
    "B = np.linalg.inv(np.eye(m) - A)\n",
    "\n",
    "eps = 0.1\n",
    "i = 5\n",
    "j = 4\n",
    "\n",
    "# YOUR CODE\n",
    "# # # # # #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmdEYVjHqxN3"
   },
   "source": [
    "**c.** (7 pts) Try to run your code for the example proposed below. Is it working well? If not, what is the problem? Is it because $\\rho(A) > 1$ or becasue $\\sigma^2 = \\infty$ or for some completely different reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2E645FPClVZj",
    "outputId": "ac0753db-7f6b-4072-89d9-f89c1ed932a9"
   },
   "outputs": [],
   "source": [
    "# test case 3\n",
    "\n",
    "np.random.seed(41)\n",
    "\n",
    "m = 200\n",
    "alpha = 0.2\n",
    "\n",
    "G = np.random.randn(m, m)\n",
    "Q = np.linalg.qr(G)[0]\n",
    "A = alpha * Q\n",
    "B = np.linalg.inv(np.eye(m) - A)\n",
    "\n",
    "eps = 0.1\n",
    "i = 0\n",
    "j = 4\n",
    "N_walks = 50000\n",
    "\n",
    "print(\"Test 3\", B[i, j], MC_solver(A, i, j, N_walks, eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yt_kBd3crneF"
   },
   "source": [
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acV1dTm8rS9M",
    "outputId": "77e47f18-576e-4ebd-a7a7-15840de30dff"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "\n",
    "# # # # # #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0i-s-I-ijexB"
   },
   "source": [
    "## Sources\n",
    "\n",
    "1. Method described here was proposed by J. von Neumann and S. M. Ulam and was published in [Matrix inversion by a Monte Carlo method](https://www.ams.org/journals/mcom/1950-04-031/S0025-5718-1950-0038138-X/home.html)\n",
    "\n",
    "2. Particular formulation that I used is from [A note on the inversion of matrices by random walks](https://www.jstor.org/stable/2002546). There and in [Solving linear algebraic equations can be interesting](https://www.ams.org/journals/bull/1953-59-04/S0002-9904-1953-09718-X/) you can find more references to other methods including the ones based on [Feynman-Kac formula](https://en.wikipedia.org/wiki/Feynman%E2%80%93Kac_formula)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0gqKcERHlQVp",
    "tTk5SLH2lQVy",
    "UfZJI0WqQnkA",
    "19uYYBL2iWcY",
    "ESIONm0Iig7G",
    "ieFM1uYFi7QV",
    "E9FQ-1E1cnTJ",
    "zhhAdK3Dcpr0",
    "O_6ZCs4xdMLl"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
