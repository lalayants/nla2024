{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hz40s6dfMSpo",
   "metadata": {
    "id": "hz40s6dfMSpo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7VQa1lwn_vCU",
   "metadata": {
    "id": "7VQa1lwn_vCU"
   },
   "source": [
    "# Linear systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FOrCXR7c_v4y",
   "metadata": {
    "id": "FOrCXR7c_v4y"
   },
   "source": [
    "## Outline\n",
    "\n",
    "1. LU decomposition as a sum of rank one matrices\n",
    "2. LU Decomposition with Pivoting\n",
    "3. Overdetermined Linear Systems (Least Squares)<br>\n",
    "    3.1 Geometry of Normal Equation <br>\n",
    "    3.2 Pseudoinverse <br>\n",
    "    3.3 Pseudoinverse via SVD <br>\n",
    "    3.4 QR approach <br>\n",
    "    3.5 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S5xfjkRB7Xbx",
   "metadata": {
    "id": "S5xfjkRB7Xbx"
   },
   "source": [
    "In many applications, we encounter **overdetermined systems** of linear equations, where there are more equations than unknowns, represented as\n",
    "\n",
    "$$ Ax = b. $$\n",
    "\n",
    "These systems often lack an exact solution.\n",
    "\n",
    "To address this, we can use the **least squares method**, which finds an approximate solution by minimizing the residual error:\n",
    "\n",
    "$$ x = \\arg\\min_x \\| Ax - b \\|_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9p1Azz6-lvmg",
   "metadata": {
    "id": "9p1Azz6-lvmg"
   },
   "source": [
    "## Overdetermined Linear Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ewELcWNpmBHs",
   "metadata": {
    "id": "ewELcWNpmBHs"
   },
   "source": [
    "Many applications lead to unsolvable linear equations $Ax = b$, where the number of equations is greater, than the number of unknowns.\n",
    "\n",
    "The least squares method chooses the solution as\n",
    "\n",
    "$$ x = \\arg\\min_x \\| Ax - b \\|_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Jx9G873_8D6W",
   "metadata": {
    "id": "Jx9G873_8D6W"
   },
   "source": [
    "### The Normal Equation\n",
    "\n",
    "The solution to the least squares problem satisfies the **normal equation**:\n",
    "\n",
    "$$ A^T A x = A^T b $$\n",
    "\n",
    "When $ A $ has full column rank, the matrix $ A^T A $ is **positive definite**, allowing us to solve this equation efficiently with **Cholesky decomposition**:\n",
    "\n",
    "$$ A^T A = R^T R $$\n",
    "\n",
    "where $ R $ is an upper triangular matrix (and $ R^T $ is lower triangular). This leads to the following set of equations:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    R^T y &= A^T b \\\\\n",
    "    R x &= y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "However, solving the normal equations directly can be numerically unstable, especially for larger problems. This approach is generally safe for small problems, but for stability, other methods are recommended. If the pivots in Gaussian elimination are small, **LU decomposition** may fail, whereas **Cholesky decomposition** remains stable in these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tLbUY-9Ys3n-",
   "metadata": {
    "id": "tLbUY-9Ys3n-"
   },
   "outputs": [],
   "source": [
    "# Cholesky Decomposition Method\n",
    "def leastsq_chol(A, b):\n",
    "    R = scipy.linalg.cholesky(A.T @ A)\n",
    "    w = scipy.linalg.solve_triangular(R, A.T @ b, trans='T')\n",
    "    return scipy.linalg.solve_triangular(R, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lm44HXeq8hIY",
   "metadata": {
    "id": "lm44HXeq8hIY"
   },
   "source": [
    "### The Pseudoinverse\n",
    "\n",
    "In linear algebra, not all matrices have an inverse, particularly when a system of equations has no solution or many solutions. The **Moore-Penrose pseudoinverse** offers a way to find an approximate solution that minimizes error, even when a unique solution doesn’t exist.\n",
    "\n",
    "In the lecture, the pseudoinverse is defined as:\n",
    "\n",
    "$$ A^{\\dagger} = \\lim_{\\alpha \\to 0} (A^T A + \\alpha I)^{-1} A^T $$\n",
    "\n",
    "Alternatively,\n",
    "\n",
    "$$ A^{\\dagger} = \\lim_{\\alpha \\to 0} A^T (A A^T + \\alpha I)^{-1} $$\n",
    "\n",
    "These limits exist even if $ (A^T A)^{-1} $ or $ (A A^T)^{-1} $ do not. Later, we will see how this relates to **Tikhonov regularization**.\n",
    "\n",
    "If $ A $ has full column rank, the pseudoinverse simplifies to:\n",
    "\n",
    "$$ A^{\\dagger} = (A^T A)^{-1} A^T $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EXqJZw_K9CAJ",
   "metadata": {
    "id": "EXqJZw_K9CAJ"
   },
   "source": [
    "### Computing the Pseudoinverse Using SVD\n",
    "\n",
    "Another way to compute the pseudoinverse is through the **Singular Value Decomposition (SVD)** $ A = U \\Sigma V^T $:\n",
    "\n",
    "$$ A^{\\dagger} = V \\Sigma^{\\dagger} U^T $$\n",
    "\n",
    "In this approach, we invert the diagonal entries of $ \\Sigma $ where possible.\n",
    "\n",
    "With the pseudoinverse $ A^{\\dagger} $, we can write the solution to $ Ax = b $ as:\n",
    "\n",
    "$$ x = A^{\\dagger} b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EiUm7unL9eAQ",
   "metadata": {
    "id": "EiUm7unL9eAQ"
   },
   "source": [
    "### Example: Solving a System Using Pseudoinverse\n",
    "\n",
    "Let’s now consider the toy matrix and system of equations:\n",
    "\n",
    "$$ \\begin{bmatrix} 1 & -1 \\\\ 2 & 1 \\\\ -1 & 3 \\end{bmatrix} x = \\begin{bmatrix} 0 \\\\ 3 \\\\ 2 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6stWGXj33df6",
   "metadata": {
    "id": "6stWGXj33df6"
   },
   "outputs": [],
   "source": [
    "# Pseudo-inverse Method using Matrix Inversion\n",
    "def leastsq_pinv(A, b):\n",
    "    return np.linalg.inv(A.T @ A) @ A.T @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GBwmg2C7vD2U",
   "metadata": {
    "id": "GBwmg2C7vD2U"
   },
   "outputs": [],
   "source": [
    "# Pseudo-inverse Method using SVD\n",
    "def leastsq_pinv_svd(A, b):\n",
    "    U, S, Vh = scipy.linalg.svd(A)\n",
    "    S_plus = np.zeros(A.shape).T\n",
    "    S_plus[:S.shape[0], :S.shape[0]] = np.linalg.inv(np.diag(S))\n",
    "    return Vh.T @ S_plus @ U.T @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37suxk1fvHFS",
   "metadata": {
    "id": "37suxk1fvHFS"
   },
   "source": [
    "Let's now return to our toy matrix and consider the system\n",
    "\n",
    "$$\\begin{bmatrix} 1 & -1 \\\\ 2 & 1 \\\\ -1 & 3 \\end{bmatrix} x =\\begin{bmatrix} 0 \\\\ 3 \\\\ 2 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sHH09DC2MENT",
   "metadata": {
    "id": "sHH09DC2MENT"
   },
   "outputs": [],
   "source": [
    "A = np.array([[1., -1.], [2., 1.], [-1., 3.]])\n",
    "b = np.array([[0.], [3.], [2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2iIrQU8H4QdX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2iIrQU8H4QdX",
    "outputId": "372db561-5347-4829-aa97-9840c8d47717"
   },
   "outputs": [],
   "source": [
    "print(\"Condition number of A:\\n\", np.linalg.cond(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iFgq7qdY-wt5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFgq7qdY-wt5",
    "outputId": "6b1fa758-90f5-476b-ac4c-90177efacd92"
   },
   "outputs": [],
   "source": [
    "A_pinv = np.linalg.pinv(A)\n",
    "x_sol = A_pinv @ b\n",
    "print(\"\\n Solution by using pseudoinverse: \\n\", x_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H-GapDCr-Qqe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "H-GapDCr-Qqe",
    "outputId": "5453f939-e6d9-41d3-e230-d155bc4efb06"
   },
   "outputs": [],
   "source": [
    "x_1 = np.linspace(-3, 3, 100)\n",
    "x_2_1 = x_1\n",
    "x_2_2 = 3. - 2. * x_1\n",
    "x_2_3 =(2. - x_1) / 3\n",
    "\n",
    "plt.plot(x_1, x_2_1)\n",
    "plt.plot(x_1, x_2_2)\n",
    "plt.plot(x_1, x_2_3)\n",
    "plt.scatter(x_sol[0], x_sol[1], c='r')\n",
    "\n",
    "plt.xlim(0., 2.)\n",
    "plt.ylim(-0.5, 2.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_jUnw-bw1ktM",
   "metadata": {
    "id": "_jUnw-bw1ktM"
   },
   "source": [
    "### QR Decomposition for Solving Least Squares Problem\n",
    "\n",
    "If $ A $ has full column rank, a **QR decomposition** exists for $ A $:\n",
    "\n",
    "$$ A = QR $$\n",
    "\n",
    "This allows us to rewrite the normal equations as:\n",
    "\n",
    "$$ R^T Q^T Q R x = R^T Q^T b $$\n",
    "\n",
    "Since $ Q^T Q = I $, this further simplifies to:\n",
    "\n",
    "$$ R x = Q^T b. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6Dx9VDfwCbv",
   "metadata": {
    "id": "d6Dx9VDfwCbv"
   },
   "outputs": [],
   "source": [
    "# QR Decomposition Method\n",
    "def leastsq_qr(A, b):\n",
    "    Q, R = scipy.linalg.qr(A, mode='economic')\n",
    "    return scipy.linalg.solve_triangular(R, Q.T @ b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439WStZCwHkX",
   "metadata": {
    "id": "439WStZCwHkX"
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hVRAIeoI1ann",
   "metadata": {
    "id": "hVRAIeoI1ann"
   },
   "outputs": [],
   "source": [
    "def SyntheticData(x, corr_col=False, noise=0.1, num_points=100):\n",
    "    A = np.random.randn(num_points, len(x) - 1)\n",
    "    A = np.hstack((A, np.ones((num_points, 1))))\n",
    "    if corr_col and len(x) > 2:\n",
    "        A[:, 2] = A[:, 1] + np.random.rand(num_points) * noise * 1e-4\n",
    "    noise = np.random.randn(num_points, 1) * noise\n",
    "    b = A @ x.reshape((-1, 1)) + noise\n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jnPFGg3Zrr6P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 783
    },
    "id": "jnPFGg3Zrr6P",
    "outputId": "9c1f19ea-9fb2-42bb-a282-182dcc13e229"
   },
   "outputs": [],
   "source": [
    "x_true = np.array([2.0, 1.0])\n",
    "\n",
    "A, b = SyntheticData(x_true, num_points=1000)\n",
    "\n",
    "x_chol = leastsq_chol(A, b)\n",
    "x_pinv = leastsq_pinv(A, b)\n",
    "x_svd = leastsq_pinv_svd(A, b)\n",
    "x_qr = leastsq_qr(A, b)\n",
    "\n",
    "print(\"Condition number of A:\\n\", np.linalg.cond(A))\n",
    "\n",
    "print(\"True coefficients:\\n\", x_true)\n",
    "print(\"Cholesky solution:\\n\", x_chol.flatten())\n",
    "print(\"Pseudo-inverse solution:\\n\", x_pinv.flatten())\n",
    "print(\"SVD solution:\\n\", x_svd.flatten())\n",
    "print(\"QR solution:\\n\", x_qr.flatten())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(A[:, 0], b, label='Data Points', color='blue', alpha=0.5)\n",
    "\n",
    "plt.plot(A[:, 0], A @ x_true, label='True Line', color='green')\n",
    "plt.plot(A[:, 0], A @ x_chol, label='Cholesky Fit', color='red')\n",
    "plt.plot(A[:, 0], A @ x_pinv, label='Pseudo-inverse Fit', color='purple')\n",
    "plt.plot(A[:, 0], A @ x_svd, label='SVD Fit', color='orange')\n",
    "plt.plot(A[:, 0], A @ x_qr, label='QR Fit', color='cyan')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('1D Linear Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P1jiYnOD5kGp",
   "metadata": {
    "id": "P1jiYnOD5kGp"
   },
   "source": [
    "Finally, let’s consider the case where $ A $ does not have full column rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4CN8zRcbrr4M",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CN8zRcbrr4M",
    "outputId": "2229451d-d1ac-49c1-80a0-76a1fda26595"
   },
   "outputs": [],
   "source": [
    "x_true = np.array([2.0, 3.0, -1.0, 1.0])\n",
    "\n",
    "A, b = SyntheticData(x_true, corr_col=True, num_points=1000)\n",
    "\n",
    "x_chol = leastsq_chol(A, b)\n",
    "x_pinv = leastsq_pinv(A, b)\n",
    "x_svd = leastsq_pinv_svd(A, b)\n",
    "x_qr = leastsq_qr(A, b)\n",
    "\n",
    "print(\"Condition number of A:\\n\", np.linalg.cond(A))\n",
    "\n",
    "print(\"True coefficients:\\n\", x_true)\n",
    "print(\"Cholesky solution:\\n\", x_chol.flatten())\n",
    "print(\"Pseudo-inverse solution:\\n\", x_pinv.flatten())\n",
    "print(\"SVD solution:\\n\", x_svd.flatten())\n",
    "print(\"QR solution:\\n\", x_qr.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ttsUsG-D_ZTV",
   "metadata": {
    "id": "ttsUsG-D_ZTV"
   },
   "source": [
    " In such cases Tikhonov regularization is more effective for finding stable solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w3-qXhYeF0__",
   "metadata": {
    "id": "w3-qXhYeF0__"
   },
   "source": [
    "**<u>Exercise</u>** Show that the problem\n",
    "\n",
    "$$ \\min \\|x'\\|_2 \\quad \\text{s.t. } x' = \\arg\\min_x \\| Ax - b \\|_2^2 $$\n",
    "\n",
    "is equivalent to the following regularization problem:\n",
    "\n",
    "$$ \\min_x \\| Ax - b \\|_2^2 + \\lambda \\|x\\|_2^2. $$\n",
    "\n",
    "Note that the analytical solution is available in this case as well:\n",
    "\n",
    "$$ x^* = (A^T A + \\lambda I)^{-1}  X^T b.$$\n",
    "\n",
    "The analytical solutions described earlier involve inverting the matrix $A^T A$ (or $A^T A + \\lambda I$), which is computationally expensive. This brings us to iterative methods, which are generally more efficient and have become the primary approach for numerous applications.\n",
    "\n",
    "**Gradient descent** is one of the most widely used optimization methods. It is important to note that the objective function (e.g., the loss function, which is the residual in our case: $\\mathcal{L}(x) = \\|Ax - b\\|_2^2$) should be differentiable with respect to the unknown $ x $. Using gradient descent, the weight vector at each step can be updated as follows:\n",
    "\n",
    "$$ x^{k+1} = x^k - \\beta_k \\nabla \\mathcal{L}(x^k) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3HOUmxpGsd1",
   "metadata": {
    "id": "f3HOUmxpGsd1"
   },
   "source": [
    "**<u>Exercise</u>** Compute the gradient of $$\\mathcal{L}_{\\lambda}(x) = \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2.$$\n",
    "\n",
    "To further improve efficiency, one could use **stochastic gradient descent (SGD)**, which computes the gradient over a randomly selected subset of data points, rather than the full dataset $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd2aIBvF1dV",
   "metadata": {
    "id": "9fd2aIBvF1dV"
   },
   "source": [
    "These ideas will be explored further in the second homework assignment... stay tuned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkf5DI4h02ks",
   "metadata": {
    "id": "bkf5DI4h02ks"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pupa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
